{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica Web Scraping 2\n",
    "\n",
    "\n",
    "1. Instalamos las dependencias \n",
    "\n",
    "    ```\n",
    "    pip install requests beautifulsoup4\n",
    "    ```\n",
    "\n",
    "2. Luego importamos las dependecias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Librería para manejo de archivos y directorios\n",
    "import requests # Librería para realizar solicitudes HTTP\n",
    "from bs4 import BeautifulSoup # Librería para parsear HTML y extraer información\n",
    "from urllib.parse import urljoin, urlparse # Librería para manejar URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creamos una funcion para crear la carpeta en donde se guardaran las imagenes descargadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_carpeta(carpeta):\n",
    "    if not os.path.exists(carpeta): # Verifica si la carpeta ya existe\n",
    "        os.makedirs(carpeta) # Crea la carpeta si no existe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Creamos otra funcion que manejará las extenciones de la img \".png\", \"jpg\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_extension_valida(url):\n",
    "    extensiones_validas = ['.png', '.jpg', '.jpeg', '.webp'] # Extensiones de imagen permitidas\n",
    "    return any(url.lower().endswith(ext) for ext in extensiones_validas) # Verifica si la extensión de la imagen es válida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Creamos otra funcion para descargar las imagenes teniendo en cuenta la URL y luego guardarlo en la carpeta, en este caso \"imagenes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descargar_imagen(url, carpeta):\n",
    "    try:\n",
    "        respuesta = requests.get(url, stream=True)  # Realiza la solicitud HTTP para descargar la imagen\n",
    "        respuesta.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "        nombre_imagen = os.path.join(carpeta, os.path.basename(urlparse(url).path))  # Obtiene el nombre de la imagen desde la URL\n",
    "        with open(nombre_imagen, 'wb') as archivo:  # Abre un archivo en modo escritura binaria\n",
    "            for chunk in respuesta.iter_content(chunk_size=8192):  # Descarga la imagen en trozos\n",
    "                archivo.write(chunk)  # Escribe cada trozo en el archivo\n",
    "        print(f\"Imagen descargada: {nombre_imagen}\")  # Imprime un mensaje confirmando la descarga\n",
    "    except requests.exceptions.RequestException as e:  # Maneja errores de solicitud\n",
    "        print(f\"No se pudo descargar la imagen {url}: {e}\")  # Imprime un mensaje de error en caso de que la imagen no se pueda descargar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Creamos otra funcion que será la principal, manejará la descarga de imagenes desde una URL epecifica y las guardara en una carpeta determinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url, carpeta='imagenes', limite=20):  # Para desactivar el límite, use \"limite=None\"\n",
    "    # Función para descargar imágenes desde una URL dada, guardándolas en una carpeta especificada y limitando la cantidad de descargas.\n",
    "    \n",
    "    # Crea la carpeta si no existe\n",
    "    crear_carpeta(carpeta)\n",
    "    contador = 0  # Inicializa el contador de imágenes descargadas\n",
    "    \n",
    "    try:\n",
    "        # Realiza la solicitud HTTP a la URL\n",
    "        respuesta = requests.get(url)\n",
    "        respuesta.raise_for_status()  # Verifica si la solicitud fue exitosa\n",
    "        soup = BeautifulSoup(respuesta.content, 'html.parser')  # Parsea el contenido HTML con BeautifulSoup\n",
    "        imagenes = soup.find_all('img')  # Encuentra todas las etiquetas <img> en el HTML\n",
    "        \n",
    "        for img in imagenes:\n",
    "            if limite is not None and contador >= limite:  # Verifica si se ha alcanzado el límite de descargas\n",
    "                break  # Si se alcanza el límite, termina el bucle\n",
    "            src = img.get('src')  # Obtiene el atributo src de la etiqueta <img>\n",
    "            if src:\n",
    "                # Manejar URLs relativas\n",
    "                src = urljoin(url, src)  # Convierte URLs relativas a absolutas\n",
    "                if es_extension_valida(src):  # Verifica si la extensión de la imagen es válida\n",
    "                    descargar_imagen(src, carpeta)  # Descarga la imagen\n",
    "                    contador += 1  # Incrementa el contador de imágenes descargadas\n",
    "                else:\n",
    "                    print(f\"Extensión no válida, se omite: {src}\")  # Imprime un mensaje si la extensión no es válida\n",
    "            else:\n",
    "                print(\"Etiqueta <img> sin atributo src encontrado, se omite\")  # Imprime un mensaje si la etiqueta <img> no tiene src\n",
    "    except requests.exceptions.RequestException as e:  # Maneja errores de solicitud\n",
    "        print(f\"Error al conectar con la página {url}: {e}\")  # Imprime un mensaje de error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Por ultimo, definimos la URL en la cual haremos scraping y llamamos a la funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.shutterstock.com/es/search/argentina-worldcup?PPC_GOO_LA_IG-621714502729=&cr=c&ds_ag=FF%3DDSA-All+Pages_AU%3DVisitors&ds_agid=58700008023276312&ds_cid=71700000100238870&ds_eid=700000001478290&gad_source=1&gclid=CjwKCAjw9cCyBhBzEiwAJTUWNQtUjvpOc3a0Auwn9UaGWajB5qEL1yXFfiz_9wgNohZe6ukdl3zqMhoCObcQAvD_BwE&gclsrc=aw.ds&kw=&utm_campaign=CO%3DLatam_LG%3DES_BU%3DIMG_AD%3DDSA_TS%3Dlggeneric_RG%3DAMER_AB%3DACQ_CH%3DSEM_OG%3DCONV_PB%3DGoogle&utm_medium=cpc&utm_source=GOOGLE'\n",
    "scraper(url, limite=20) # Llama a la función scraper con la URL y el límite de descargas\n",
    "                        # Para desactivar el limite, utilice \"limite=None\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
